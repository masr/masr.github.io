
<!DOCTYPE html>
<html lang="zh-cn">
    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Suhan&#39;s blog">
    <title>Namenode的手动failover机制的研究 - Suhan&#39;s blog</title>
    <meta name="author" content="Suhan Mao">
    
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Suhan Mao","sameAs":[],"image":"me.jpg"},"articleBody":"这几天尝试在公司的集群做namenode的平顺failover，结果遇到了很多问题，debug了好几天。最后终于将问题的脉络梳理出来，觉得很有必要分享一下。当时的场景是这样的：\n集群是标准的namenode HA模式，配置了namenode的自动failover。其架构图如下：\n\n有两个Namenode，最初的active是nn1，standby是nn2。每台NN上都起了zkfc，分别是zkfc1和zkfc2。\n平时我们做failover的时候，往往会直接将active的NN kill掉，然后ZKFC会定时检查NN的健康状态，如果发现当前NN一直不响应，则会退出选举，从而对方的ZKFC会赢得选举并且辅助其NN成为active。但是这个过程的风险在于如果standby变成active后处于非健康的状态或者因为别的原因crash了，那么整个集群就不可用了，因为在一个大规模几千台节点的集群中，重新启动NN是需要超过一个小时的。\n这次我打算使用平顺的failover，也就是graceful failover。根据官方文档，我们可以执行命令hdfs haadmin -failover nn1 nn2。其中nn1是当前的active NN的id号，nn2是当前的standby的NN的id号。\n小心翼翼的敲完命令执行，发现命令会一直hang在那里，然后过了大约一分钟报错，会报错，错误显示Failed to become active。当时我就比较慌，于是打开两台NN的页面，发现两台NN目前都是standby状态，等了大半分钟，发现nn2 crash了，又等了将近一分钟，nn1重新成为了active。这意味着我们的命令失败了，不仅没有failover成功，还把原来的standby NN杀掉了。\n于是开始了我辛苦的DEBUG之旅。。。。。。\n整个DEBUG过程是复杂和繁琐的，于是我直接讲结果和结论吧！\n我们先了解下NN是如何实现主备选举的。简单的就是两个ZKFC作为zookeeper client都会尝试在zookeeper的某个路径下创建类型为EPHEMERAL的znode，也就是临时节点，我们称之为zkLock znode。谁首先创建了该znode，就相当于拥有了该znode，并尝试成为active。当拥有该znode的ZKFC的zookeeper连接断开后，这个znode就会自动被回收掉。而双方的ZKFC都会使用zookeeper的watch机制监控该znode的状态变化，当znode被回收掉，另外一方的ZKFC会尝试创建该znode，从而进行主从切换。\nzookeeper中还有一个重要的znode是breadCrumb znode，属于PERSISTENT类型。当ZKFC主动放弃active状态或者ZKFC尝试成为active时会创建该节点，并将自己的身份信息写入改节点。我们会在下面详细讲解这个节点的作用。\nhdfs-site.xml可以配置ha.zookeeper.parent-znode作为zookeeper中ZKFC使用的父znode。\nzkLock znode的路径是 &lt;ha.zookeeper.parent-znode&gt;/ActiveStandbyElectorLock\nbreadCrumb znode的路径就是 &lt;ha.zookeeper.parent-znode&gt;/ActiveBreadCrumb\n下面是执行failover时候的正常情况下的时序图：\n\n我来详细的介绍时序图中的每一步。\n\nCLI机器执行hdfs haadmin -failover nn1 nn2命令，其实是通过协议ZKFCProtocol中的gracefulFailover方法发送RPC请求控制ZKFC进行failover操作。因为nn2在命令参数的最后面，所以对应的zkfc2会接收到这个命令。\nzkfc2会向zkfc1通过ZKFCProtocol中的cedeActive(timeout)方法发送RPC请求给zkfc1。该方法的意思是要求对方退出选举，并且在timeout时间内都不得参加选举。\nzkfc1会通过HAServiceProtocol中的transitionToStandby方法发送RPC请求给nn1，请求其成为standby。nn1成功返回。\nzkfc1会删除zookeeper中位于breadCrumbPath路径的znode，表示已主动放弃active。\nzkfc1主动断开与zookeeper的连接。这时候一个隐含的行为是zkLockPath作为临时节点被删除了。\n因为ZKFC会在zkLockPath设置watch事件，所以zkfc2很快会收到zookeeper发来的消息，得知zkLockPath被释放了。\nzkfc2在zookeeper中创建zkLock znode。\nzkfc2在zookeeper中创建breadCrumb znode，并写入身份信息。\nzkfc2会通过HAServiceProtocol中的transitionToActive方法发送RPC请求给nn2，请求其成为active。nn2成功返回。此时failover其实已经成功。\nzkfc2发送cedeActive(-1)给zkfc1，请求其立即参加选举。\nzkfc1再次发送transitionToStandby给nn1。\nCLI机器得到命令的成功返回\n\n然而在我真正线上的场景下，遇到了一些异常，甚至遇到了nn crash的情形。其真正的时序图远比正常情况复杂得多。时序图如下：\n\n和正常情况不同的步骤发生在第9步，我们从这里开始讲解。根据zkfc2的日志，这边出现了SocketTimeoutException。\njava.net.SocketTimeoutException: Call From /&lt;nn2_ip&gt;to :8030 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/&lt;nn2_ip&gt;:49697 remote=/&lt;nn2_ip&gt;:8030]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n\n这意味着zkfc2在对于nn2的transitionToActive的RPC请求超时了，超时时间是60s。我们先不去探究为何超时。我将在后面给出答案。\n因为ZKFC会自动进行zookeeper的重连，从而zkfc1又会加入到选举中。但是此时的zkLock znode被zkfc2占据，所以zkfc1会尝试成为standby。然而就是连zkfc1对于nn1的transitionToStandby也超时了。这发生在第11步。\n\nCLI会提早接收到failover命令的结果，显示failover失败。\nzkfc2断开与zookeeper的连接，此时zkLock znode又被释放了。\nzkfc2重新连接zookeeper。\nzookeeper发送zkLock znode被释放的消息给zkfc1。\nzkfc1创建zkLock znode。\nzkfc1访问breadCrumb znode的数据，判断是否其中的身份信息与自己相符。因为这个节点上一次是被zkfc2创建的，所以身份信息是zkfc2的。这时，zkfc1认为zkfc2曾经尝试成为active。于是决定把nn2强制杀掉。\n这就是关键的Fence步骤，我们在hdfs-site.xml配置了sshfence选项，zkfc1会尝试ssh登录nn2所在的节点，查看占用namenode端口所在的进程，并发送SIGKILL信号杀死该namenode。这里对应的fencing的配置是dfs.ha.fencing.methods，可以配置成sshfence和shell。\nzkfc1创建breadCrumb znode，并写入自己的身份信息。\nzkfc1发送transitionActive给nn1，但是仍然遇到SocketTimeoutException。\nzkfc1终于在重试若干次transitionActive后将nn1置为active。\nzkfc2其实很早就加入了重新选举。但是因为zkLock znode已经被zkfc1占据，所以他只能尝试成为standby。但是仍然遇到SocketTimeoutException。\nzkfc2终于在重试若干次transitionToStandby后将nn2置为standby。\n\n现在的疑问是为何ZKFC和NN交互会socketTimeout。我们通过监控图查看nn2的service rpc端口的callQueueLength发现的确会有一段时间callqueue打满的情况。NN配置的service port的handler数目是55个，总的queue大小是5500 = 100 × 55\n\n于是我又选了一个时间复现了整个过程，并且打出了当时nn2的jstack。发现55个handler全部都是处于如下的状态：12345678910111213141516171819202122&quot;IPC Server handler 52 on 8030&quot; #731 daemon prio=5 os_prio=0 tid=0x00007f4a2deb8800 nid=0x213b waiting on condition [0x00007f212e276000]   java.lang.Thread.State: WAITING (parking)        at sun.misc.Unsafe.park(Native Method)        - parking to wait for  &lt;0x00007f2557b10400&gt; (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)        at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock(FSNamesystem.java:1477)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleHeartbeat(FSNamesystem.java:4641)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1395)        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:114)        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:29064)        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:422)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)\n所有的handler都是在处理NameNodeRpcServer.sendHeartbeat，也就数处理datanode的心跳。默认配置下，datanode每隔3s就会向namenode发送一次心跳。但是这个请求是很轻量级的。而且我发现所有的handler都在等锁，那就要看看是谁把锁给占了，发现是Edit Log Tailer占据了写锁。\n123456789101112131415161718192021222324&quot;Edit log tailer&quot; #735 prio=5 os_prio=0 tid=0x00007f4a2dec2800 nid=0x213f runnable [0x00007f212de71000]   java.lang.Thread.State: RUNNABLE        at org.apache.hadoop.hdfs.server.namenode.INodeFile.computeQuotaUsage(INodeFile.java:581)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:887)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuota(FSImage.java:868)        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:851)        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:818)        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284)        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:360)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1689)        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:447)        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)\n在EditLogTailer代码中在loadEdits之前会获得写锁。123456try&#123;namesystem.writeLockInterruptibly();// load edit logs&#125; finally &#123;    namesystem.writeUnlock();&#125;\n运行这一段代码的时间间隔配置在dfs.ha.tail-edits.period，默认是60s。这边是每次运行完loadEdits()之后休眠的时间。在我们的场景下，loadEdits显然是运行了太长的时间了。而写锁是排它锁，所以读锁会被block。而在loadEdits运行完之后，我们就有60s的时间执行其他handler中的NameNodeRpcServer.sendHeartbeat。\n根据观察，EditLogTailer.loadEdits()需要执行大概超过1分钟的时间，在此阶段所有的handler都被锁堵住，从而所有的在service port上的请求都无法被处理。新的请求要么就是在call queue里面，要么就是根本进不了call queue。从而超过了Timeout的时间。\n而且我们在nn2的日志中发现了如下的log：2018-07-10 00:30:22,736 INFO org.apache.hadoop.ipc.Server: IPC Server handler 30 on 8030: skipped org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from 10.103.108.201:34576 Call#61113 Retry#0\n\n这段代码如下是在org.apache.hadoop.ipc.Server.Handler.run()中，也就是请求从call queue进入到了handler中。1234if (!call.connection.channel.isOpen()) &#123;     LOG.info(Thread.currentThread().getName() + \": skipped \" + call);     continue;&#125;\n因为ZKFC作为RPC的客户端设置有Socket的超时时间60s。而transitionToActive的等待时间超过了60s，所以客户端主动的将TCP连接断开。所以当请求出queue进入到handler的时候，其实这个时候NIO的channel已经断开了，所以会进入到这个分支，也佐证了我的观点。\n目前只有Standby NN才会执行EditLogTailer.loadEdits()，所以这就是为什么一旦NN成为standby就会出现无法响应的状态。当然了，社区已经有了对应的patch，HDFS-6763解决这个问题，就是去除掉Standby NN在每次loadEdits时候处理quota的代码。如果打上这个patch，应该就不会出现长期占写锁的情况从而RPC不会超时，failover也能平顺了。\n","dateCreated":"2018-07-12T09:55:43+08:00","dateModified":"2018-07-12T09:55:43+08:00","datePublished":"2018-07-12T09:55:43+08:00","description":"这几天尝试在公司的集群做namenode的平顺failover，结果遇到了很多问题，debug了好几天。最后终于将问题的脉络梳理出来，觉得很有必要分享一下。","headline":"Namenode的手动failover机制的研究","image":["namenode_ha_architect.png"],"mainEntityOfPage":{"@type":"WebPage","@id":"http://suhan.io/namenode-failover-troubleshooting/"},"publisher":{"@type":"Organization","name":"Suhan Mao","sameAs":[],"image":"me.jpg","logo":{"@type":"ImageObject","url":"me.jpg"}},"url":"http://suhan.io/namenode-failover-troubleshooting/","keywords":"hadoop, namenode, HDFS, zookeeper","thumbnailUrl":"namenode_ha_architect.png"}</script>
    <meta name="description" content="这几天尝试在公司的集群做namenode的平顺failover，结果遇到了很多问题，debug了好几天。最后终于将问题的脉络梳理出来，觉得很有必要分享一下。">
<meta name="keywords" content="hadoop,namenode,HDFS,zookeeper">
<meta property="og:type" content="blog">
<meta property="og:title" content="Namenode的手动failover机制的研究">
<meta property="og:url" content="http://suhan.io/namenode-failover-troubleshooting/index.html">
<meta property="og:site_name" content="Suhan&#39;s blog">
<meta property="og:description" content="这几天尝试在公司的集群做namenode的平顺failover，结果遇到了很多问题，debug了好几天。最后终于将问题的脉络梳理出来，觉得很有必要分享一下。">
<meta property="og:locale" content="zh-cn">
<meta property="og:image" content="http://suhan.io/namenode-failover-troubleshooting/namenode_ha_architect.png">
<meta property="og:image" content="http://suhan.io/namenode-failover-troubleshooting/graceful_failover_normal_case.png">
<meta property="og:image" content="http://suhan.io/namenode-failover-troubleshooting/graceful_failover_timeout_case.png">
<meta property="og:image" content="http://suhan.io/namenode-failover-troubleshooting/call_queue_length.png">
<meta property="og:updated_time" content="2018-07-12T01:55:43.049Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Namenode的手动failover机制的研究">
<meta name="twitter:description" content="这几天尝试在公司的集群做namenode的平顺failover，结果遇到了很多问题，debug了好几天。最后终于将问题的脉络梳理出来，觉得很有必要分享一下。">
<meta name="twitter:image" content="http://suhan.io/namenode-failover-troubleshooting/namenode_ha_architect.png">
    
    
        
    
    
        <meta property="og:image" content="http://suhan.io../assets/images/me.jpg"/>
    
    
        <meta property="og:image" content="http://suhan.ionamenode_ha_architect.png"/>
        <meta class="swiftype" name="image" data-type="enum" content="http://suhan.ionamenode_ha_architect.png" />
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="../assets/css/style-llrvkv9mzcsp8rdteeaxqxtegiuxeuq9ppf2cknr05vhsrjlzvxq1p6fvltw.min.css">
    <!--STYLES END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    

    
</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a class="header-title-link" href="../ ">Suhan&#39;s blog</a>
    </div>
    
        
            <a  class="header-right-picture "
                href="#about">
        
        
            <img class="header-picture" src="../assets/images/me.jpg" alt="作者的图片"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="../#about">
                    <img class="sidebar-profile-picture" src="../assets/images/me.jpg" alt="作者的图片"/>
                </a>
                <h4 class="sidebar-profile-name">Suhan Mao</h4>
                
                    <h5 class="sidebar-profile-bio"><p>彼节者有间，而刀刃者无厚，以无厚入有间，恢恢乎其于游刃必有余地矣</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../ "
                            
                            title="首页"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">首页</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../all-categories"
                            
                            title="分类"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">分类</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../all-tags"
                            
                            title="标签"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">标签</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../all-archives"
                            
                            title="归档"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">归档</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                            title="搜索"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-search" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">搜索</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                            title="关于"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-question" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">关于</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="../atom.xml"
                            
                            title="RSS"
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-rss" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">RSS</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            Namenode的手动failover机制的研究
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2018-07-12T09:55:43+08:00">
	
		    7月 12, 2018
    	
    </time>
    
        <span>发布在 </span>
        
    <a class="category-link" href="../categories/大数据/">大数据</a>, <a class="category-link" href="../categories/大数据/hadoop/">hadoop</a>


    
</div>

    
</div>

    
    <div class="post-content markdown">
        <div class="main-content-wrap">
            <p>这几天尝试在公司的集群做namenode的平顺failover，结果遇到了很多问题，debug了好几天。最后终于将问题的脉络梳理出来，觉得很有必要分享一下。<br><a id="more"></a><br>当时的场景是这样的：</p>
<p>集群是标准的namenode HA模式，配置了namenode的自动failover。其架构图如下：</p>
<p><img src="namenode_ha_architect.png" alt="Namenode HA Architect"></p>
<p>有两个Namenode，最初的active是nn1，standby是nn2。每台NN上都起了zkfc，分别是zkfc1和zkfc2。</p>
<p>平时我们做failover的时候，往往会直接将active的NN kill掉，然后ZKFC会定时检查NN的健康状态，如果发现当前NN一直不响应，则会退出选举，<br>从而对方的ZKFC会赢得选举并且辅助其NN成为active。但是这个过程的风险在于如果standby变成active后处于非健康的状态或者因为别的原因crash了，<br>那么整个集群就不可用了，因为在一个大规模几千台节点的集群中，重新启动NN是需要超过一个小时的。</p>
<p>这次我打算使用平顺的failover，也就是graceful failover。根据官方文档，我们可以执行命令<code>hdfs haadmin -failover nn1 nn2</code>。<br>其中nn1是当前的active NN的id号，nn2是当前的standby的NN的id号。</p>
<p>小心翼翼的敲完命令执行，发现命令会一直hang在那里，然后过了大约一分钟报错，会报错，错误显示<code>Failed to become active</code>。当时我就比较慌，于是打开两台NN的页面，<br>发现两台NN目前都是standby状态，等了大半分钟，发现nn2 crash了，又等了将近一分钟，nn1重新成为了active。<br>这意味着我们的命令失败了，不仅没有failover成功，还把原来的standby NN杀掉了。</p>
<p>于是开始了我辛苦的DEBUG之旅。。。。。。</p>
<p>整个DEBUG过程是复杂和繁琐的，于是我直接讲结果和结论吧！</p>
<p>我们先了解下NN是如何实现主备选举的。简单的就是两个ZKFC作为zookeeper client都会尝试在zookeeper的某个路径下创建类型为EPHEMERAL的znode，<br>也就是临时节点，我们称之为zkLock znode。谁首先创建了该znode，就相当于拥有了该znode，并尝试成为active。当拥有该znode的ZKFC的zookeeper连接断开后，这个znode就会自动被回收掉。<br>而双方的ZKFC都会使用zookeeper的watch机制监控该znode的状态变化，当znode被回收掉，另外一方的ZKFC会尝试创建该znode，从而进行主从切换。</p>
<p>zookeeper中还有一个重要的znode是breadCrumb znode，属于PERSISTENT类型。当ZKFC主动放弃active状态或者ZKFC尝试成为active时会创建该节点，<br>并将自己的身份信息写入改节点。我们会在下面详细讲解这个节点的作用。</p>
<p>hdfs-site.xml可以配置ha.zookeeper.parent-znode作为zookeeper中ZKFC使用的父znode。</p>
<p>zkLock znode的路径是 <code>&lt;ha.zookeeper.parent-znode&gt;/ActiveStandbyElectorLock</code></p>
<p>breadCrumb znode的路径就是 <code>&lt;ha.zookeeper.parent-znode&gt;/ActiveBreadCrumb</code></p>
<p>下面是执行failover时候的正常情况下的时序图：</p>
<p><img src="graceful_failover_normal_case.png" alt="Namenode Graceful Failover"></p>
<p>我来详细的介绍时序图中的每一步。</p>
<ol>
<li>CLI机器执行<code>hdfs haadmin -failover nn1 nn2</code>命令，其实是通过协议ZKFCProtocol中的gracefulFailover方法发送RPC请求控制ZKFC进行failover操作。<br>因为nn2在命令参数的最后面，所以对应的zkfc2会接收到这个命令。</li>
<li>zkfc2会向zkfc1通过ZKFCProtocol中的cedeActive(timeout)方法发送RPC请求给zkfc1。该方法的意思是要求对方退出选举，并且在timeout时间内都不得参加选举。</li>
<li>zkfc1会通过HAServiceProtocol中的transitionToStandby方法发送RPC请求给nn1，请求其成为standby。nn1成功返回。</li>
<li>zkfc1会删除zookeeper中位于breadCrumbPath路径的znode，表示已主动放弃active。</li>
<li>zkfc1主动断开与zookeeper的连接。这时候一个隐含的行为是zkLockPath作为临时节点被删除了。</li>
<li>因为ZKFC会在zkLockPath设置watch事件，所以zkfc2很快会收到zookeeper发来的消息，得知zkLockPath被释放了。</li>
<li>zkfc2在zookeeper中创建zkLock znode。</li>
<li>zkfc2在zookeeper中创建breadCrumb znode，并写入身份信息。</li>
<li>zkfc2会通过HAServiceProtocol中的transitionToActive方法发送RPC请求给nn2，请求其成为active。nn2成功返回。此时failover其实已经成功。</li>
<li>zkfc2发送cedeActive(-1)给zkfc1，请求其立即参加选举。</li>
<li>zkfc1再次发送transitionToStandby给nn1。</li>
<li>CLI机器得到命令的成功返回</li>
</ol>
<p>然而在我真正线上的场景下，遇到了一些异常，甚至遇到了nn crash的情形。其真正的时序图远比正常情况复杂得多。时序图如下：</p>
<p><img src="graceful_failover_timeout_case.png" alt="Namenode Graceful Failover With Exception"></p>
<p>和正常情况不同的步骤发生在第9步，我们从这里开始讲解。<br>根据zkfc2的日志，这边出现了SocketTimeoutException。</p>
<blockquote><p>java.net.SocketTimeoutException: Call From <nn2>/&lt;nn2_ip&gt;to <nn2>:8030 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/&lt;nn2_ip&gt;:49697 remote=<nn2>/&lt;nn2_ip&gt;:8030]; For more details see:  <a href="http://wiki.apache.org/hadoop/SocketTimeout" target="_blank" rel="noopener">http://wiki.apache.org/hadoop/SocketTimeout</a></nn2></nn2></nn2></p>
</blockquote>
<p>这意味着zkfc2在对于nn2的transitionToActive的RPC请求超时了，超时时间是60s。我们先不去探究为何超时。我将在后面给出答案。</p>
<p>因为ZKFC会自动进行zookeeper的重连，从而zkfc1又会加入到选举中。但是此时的zkLock znode被zkfc2占据，所以zkfc1会尝试成为standby。<br>然而就是连zkfc1对于nn1的transitionToStandby也超时了。这发生在第11步。</p>
<ol start="12">
<li>CLI会提早接收到failover命令的结果，显示failover失败。</li>
<li>zkfc2断开与zookeeper的连接，此时zkLock znode又被释放了。</li>
<li>zkfc2重新连接zookeeper。</li>
<li>zookeeper发送zkLock znode被释放的消息给zkfc1。</li>
<li>zkfc1创建zkLock znode。</li>
<li>zkfc1访问breadCrumb znode的数据，判断是否其中的身份信息与自己相符。因为这个节点上一次是被zkfc2创建的，所以身份信息是zkfc2的。<br>这时，zkfc1认为zkfc2曾经尝试成为active。于是决定把nn2强制杀掉。</li>
<li>这就是关键的Fence步骤，我们在hdfs-site.xml配置了sshfence选项，zkfc1会尝试ssh登录nn2所在的节点，查看占用namenode端口所在的进程，<br>并发送SIGKILL信号杀死该namenode。这里对应的fencing的配置是<code>dfs.ha.fencing.methods</code>，可以配置成sshfence和shell。</li>
<li>zkfc1创建breadCrumb znode，并写入自己的身份信息。</li>
<li>zkfc1发送transitionActive给nn1，但是仍然遇到SocketTimeoutException。</li>
<li>zkfc1终于在重试若干次transitionActive后将nn1置为active。</li>
<li>zkfc2其实很早就加入了重新选举。但是因为zkLock znode已经被zkfc1占据，所以他只能尝试成为standby。但是仍然遇到SocketTimeoutException。</li>
<li>zkfc2终于在重试若干次transitionToStandby后将nn2置为standby。</li>
</ol>
<p>现在的疑问是为何ZKFC和NN交互会socketTimeout。我们通过监控图查看nn2的service rpc端口的callQueueLength发现的确会有一段时间callqueue打满的情况。<br>NN配置的service port的handler数目是55个，总的queue大小是5500 = 100 × 55</p>
<p><img src="call_queue_length.png" alt="Namenode Call Queue Length"></p>
<p>于是我又选了一个时间复现了整个过程，并且打出了当时nn2的jstack。发现55个handler全部都是处于如下的状态：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&quot;IPC Server handler 52 on 8030&quot; #731 daemon prio=5 os_prio=0 tid=0x00007f4a2deb8800 nid=0x213b waiting on condition [0x00007f212e276000]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">        at sun.misc.Unsafe.park(Native Method)</span><br><span class="line">        - parking to wait for  &lt;0x00007f2557b10400&gt; (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)</span><br><span class="line">        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)</span><br><span class="line">        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)</span><br><span class="line">        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)</span><br><span class="line">        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)</span><br><span class="line">        at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readLock(FSNamesystem.java:1477)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.handleHeartbeat(FSNamesystem.java:4641)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1395)</span><br><span class="line">        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:114)</span><br><span class="line">        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:29064)</span><br><span class="line">        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)</span><br><span class="line">        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)</span><br><span class="line">        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206)</span><br><span class="line">        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)</span><br><span class="line">        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)</span><br></pre></td></tr></table></figure></p>
<p>所有的handler都是在处理<code>NameNodeRpcServer.sendHeartbeat</code>，也就数处理datanode的心跳。默认配置下，datanode每隔3s就会向namenode发送一次心跳。<br>但是这个请求是很轻量级的。而且我发现所有的handler都在等锁，那就要看看是谁把锁给占了，发现是Edit Log Tailer占据了写锁。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&quot;Edit log tailer&quot; #735 prio=5 os_prio=0 tid=0x00007f4a2dec2800 nid=0x213f runnable [0x00007f212de71000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.INodeFile.computeQuotaUsage(INodeFile.java:581)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:887)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuotaRecursively(FSImage.java:883)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.updateCountForQuota(FSImage.java:868)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:851)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:818)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at javax.security.auth.Subject.doAs(Subject.java:360)</span><br><span class="line">        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1689)</span><br><span class="line">        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:447)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)</span><br></pre></td></tr></table></figure>
<p>在EditLogTailer代码中在loadEdits之前会获得写锁。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>&#123;</span><br><span class="line">namesystem.writeLockInterruptibly();</span><br><span class="line"><span class="comment">// load edit logs</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    namesystem.writeUnlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>运行这一段代码的时间间隔配置在<code>dfs.ha.tail-edits.period</code>，默认是60s。这边是每次运行完loadEdits()之后休眠的时间。在我们的场景下，loadEdits显然是运行了太长的时间了。<br>而写锁是排它锁，所以读锁会被block。而在loadEdits运行完之后，我们就有60s的时间执行其他handler中的<code>NameNodeRpcServer.sendHeartbeat</code>。</p>
<p>根据观察，<code>EditLogTailer.loadEdits()</code>需要执行大概超过1分钟的时间，在此阶段所有的handler都被锁堵住，从而所有的在service port上的请求都无法被处理。<br>新的请求要么就是在call queue里面，要么就是根本进不了call queue。从而超过了Timeout的时间。</p>
<p>而且我们在nn2的日志中发现了如下的log：<br><blockquote><p>2018-07-10 00:30:22,736 INFO org.apache.hadoop.ipc.Server: IPC Server handler 30 on 8030: skipped org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from 10.103.108.201:34576 Call#61113 Retry#0</p>
</blockquote></p>
<p>这段代码如下是在<code>org.apache.hadoop.ipc.Server.Handler.run()</code>中，也就是请求从call queue进入到了handler中。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!call.connection.channel.isOpen()) &#123;</span><br><span class="line">     LOG.info(Thread.currentThread().getName() + <span class="string">": skipped "</span> + call);</span><br><span class="line">     <span class="keyword">continue</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>因为ZKFC作为RPC的客户端设置有Socket的超时时间60s。而transitionToActive的等待时间超过了60s，所以客户端主动的将TCP连接断开。<br>所以当请求出queue进入到handler的时候，其实这个时候NIO的channel已经断开了，所以会进入到这个分支，也佐证了我的观点。</p>
<p>目前只有Standby NN才会执行<code>EditLogTailer.loadEdits()</code>，所以这就是为什么一旦NN成为standby就会出现无法响应的状态。<br>当然了，社区已经有了对应的patch，<a href="https://issues.apache.org/jira/browse/HDFS-6763" target="_blank" rel="noopener">HDFS-6763</a>解决这个问题，<br>就是去除掉Standby NN在每次loadEdits时候处理quota的代码。如果打上这个patch，应该就不会出现长期占写锁的情况从而RPC不会超时，failover也能平顺了。</p>

            

        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">标签</span><br/>
                
    <a class="tag tag--primary tag--small t-link" href="../tags/HDFS/">HDFS</a> <a class="tag tag--primary tag--small t-link" href="../tags/hadoop/">hadoop</a> <a class="tag tag--primary tag--small t-link" href="../tags/namenode/">namenode</a> <a class="tag tag--primary tag--small t-link" href="../tags/zookeeper/">zookeeper</a>

            </div>
        
        
        
    </div>
</article>



                
<footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2018 Suhan Mao. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <img id="about-card-picture" src="../assets/images/me.jpg" alt="作者的图片"/>
        
            <h4 id="about-card-name">Suhan Mao</h4>
        
            <div id="about-card-bio"><p>彼节者有间，而刀刃者无厚，以无厚入有间，恢恢乎其于游刃必有余地矣</p>
</div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>eBay Hadoop资深工程师</p>

            </div>
        
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('../assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="../assets/js/script-zhs8xj8td1gdbulwlxvizwj0sae4nsku0hr74cj6mgyivqwrcygzceusggqu.min.js"></script>
<!--SCRIPTS END--><!-- hexo-inject:begin --><!-- hexo-inject:end -->



    </body>
</html>
